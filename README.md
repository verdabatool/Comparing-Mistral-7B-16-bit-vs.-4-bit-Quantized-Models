# Comparing-Mistral-7B-16-bit-vs.-4-bit-Quantized-Models
This repository explores the performance differences between a 16-bit and a 4-bit quantized version of the Mistral-7B model. I have analyzed inference speed, memory consumption, and overall efficiency.
